# 선형 회귀
- 피쳐 데이터와 타깃 데이터를 가지고 학습$_{Training}$을 통해 regression (선형회귀 모델 ⇒ 식을 만든다)
- 학습된 데이터를 기반으로 입력된 값을 통해 예측결과 출력
- Linear 한 규칙으로 모델 설립 → 세상은 linear한 것들이 많음 (설명가능)

## 회귀

training data 를 이용하여 데이터의 특성과 상관관계 등을 파악하고 

그 결과를 바탕으로 Training Data 에 없는 미지의 데이터가 주어졌을 때 (predict) 

그 결과를 연속적인 값으로 예측하는 것

### 학습

입력 x 와 출력 y 의 비례관계

→ y = Wx + b (일차함수 형태로 training data를 나타낼 수 있다) → 각각 데이터에 따라 여러개의 일차함수가 있다

- 우리는 여기서 각 데이터들의 가장 최적의 y = Wx + b 를 찾아야하며 이 W와 b를 찾는 과정을 학습이라 한다.

### 오차

최적의 일차함수를 찾는 기준

- 손실함수 MSE 사용 (평균 제곱 오차) 이를 손실함수라 한다.

→ 임의의 W 가중치와 b 바이어스가 있을 때 함수 출력값과 training 데이터 값의 차이 제곱이 전체 training data 에서 가장 적게 나타나는 오차가 적게 나타나는

차이의 제곱 평균을 낸 것을 오차라 한다 → 이것을 가장 적게 만들어주는 W 가중치와 b 바이어스를 찾는 것이 Linear regression model 의 목표이다.

### 단순 선형 회귀 분석

$$
y=Wx+b
$$

### 다중 선형 회귀 분석

$$
y = W_1x_1 + W_2x_2 + W_3x_3 +. . .W_nx_n+b
$$

보통 행렬로 표현한다. 

## 가설세우기

주어진 데이터 셋에 예측을 위한 가장 좋은 가설을 세우는 것을 위한 식을 하나 세우는 것을 가설을 세운다고 한다. 

$$
H(x) = Wx+b 
$$

### 손실함수,비용함수, 목적함수

선형회귀에서 최적의 파라미터를 평가하는 것

$Cost Function$이라 함 (선과 실제 데이터의 거리)

$$
(H(x)-y)^2 
$$

보통 손실함수로 MSE 평균제곱오차가 많이 쓰인다.

$$
cost(W,b) = \frac{1}{m}\sum_{i=1}^{m}(H(x^{(i)})-y^{(i)})^2
$$

 여러개의 데이터가 있으면 각 데이터들의 거리의 평균 = 코스트

최적의 파라미터는 (W,b)의 최소 코스트값 → cost function 으로